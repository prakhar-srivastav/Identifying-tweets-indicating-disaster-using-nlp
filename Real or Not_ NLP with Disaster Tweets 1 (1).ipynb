{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"! pip3 install textaugment\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import re\n\nimport pandas as pd\nimport numpy as np\nimport csv\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_file = '../input/nlp-getting-started/train.csv'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **We define the number of words in a sentence to a fix value**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmax_sequence_length = 32","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **This is the max vocabulary size**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmax_words = 3000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Size of Embedding matrix**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nembedding_size = 32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_file = '/kaggle/working/model.h5'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer_file = '/kaggle/working/tokenizer.pickle'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_classes = 2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Follwing steps are used for cleaning the text**\n\n* **Replaced URL with the word 'Link'**\n* **Every symbol other than A-Z,a-z,0-9,(,),?,!,'**\n* **Comman type of contractions are expanded**\n* **Underscores are removed**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cleaning the text\ndef clean_str(string):\n    string = re.sub(r'http\\S+', 'link', string) # replace links by generic text link\n    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n    \n    string = re.sub(r\"\\'s\", \" \\'s\", string)\n    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n    string = re.sub(r\"\\'re\", \" \\'re\", string)\n    string = re.sub(r\"\\'d\", \" \\'d\", string)\n    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n    string = re.sub(r\",\", \" , \", string)\n    string = re.sub(r\"!\", \" ! \", string)\n    string = re.sub(r\"\\(\", \" \\( \", string)\n    string = re.sub(r\"\\)\", \" \\) \", string)\n    string = re.sub(r\"\\?\", \" \\? \", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n\n    cleanr = re.compile('<.*?>')\n\n    string = re.sub(r'\\d+', '', string)\n    string = re.sub(cleanr, '', string)\n    string = re.sub(\"'\", '', string)\n    string = re.sub(r'\\W+', ' ', string)\n    string = string.replace('_', '')\n\n    return string.strip().lower()\n\n\ncleaned_str = clean_str('The car is good.')\ncleaned_str","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **The stopwords are removed. For example,(a,an,the) etc**\n# **At least for classification problem such as this one, they are not necessary.** "},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\n\ndef remove_stopwords(word_list):\n    no_stop_words = [w for w in word_list if not w in stop_words]\n    return no_stop_words\n\n\nremove_stopwords(cleaned_str.split(\" \"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from textaugment import EDA\n\nt = EDA()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(train_file, sep=',', header=0, quotechar='\"')\n\ndata = data[['text', 'target']]\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.countplot(data.target)\nplt.xlabel('Target')\nplt.title('Number of disaster tweets')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Augmentation used are-**\n\n* **random_deletion**\n* **random_swap**\n* **synonym_replacement**\n* **random_insertion**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# text cleaning\ndata['text'] = data['text'].apply(lambda x: clean_str(x))\n\nsequences = []\ntargets = []\n\nfor index, row in data.iterrows():\n    seqs = []\n    text = row['text']\n\n    # if empty text, skipping to next row data\n    if not text:\n        continue\n\n    seqs.append(text)\n    \n    # apply data augmentation\n    \n    # random deletion\n    seq2 = t.random_deletion(text, p=0.2)\n    if type(seq2) == type([]):\n        seqs.append(seq2[0])\n    else:\n        seqs.append(seq2)\n\n    # random swap\n    if len(text) > 1:\n        seqs.append(t.random_swap(text))\n\n    # synonym replacement and random insertion\n    for i in range(2):\n        seqs.append(t.synonym_replacement(text))    \n        try:\n            seqs.append(t.random_insertion(text))\n        except:\n            pass\n\n    \n    \"\"\"\n    All sequence variations created in the data augmentation process are grouped in bags. \n    This is important to avoid that in the process of splitting the data, variations of \n    the same sequence are allocated in different sets. For example, an X variation of \n    sequence A falls in the training set and an Y variation of sequence A falls in the test set.\n    \"\"\"\n    sequence_group = []\n    target_group = []\n\n    target = row['target']\n\n    for sequence in seqs: \n        word_list = text_to_word_sequence(sequence)\n        \n        # remove stop words\n        no_stop_words = remove_stopwords(word_list)\n        \n        if not no_stop_words:\n            continue\n\n        sequence_group.append(\" \".join(no_stop_words))\n        target_group.append(target)\n\n    sequences.append(sequence_group)\n    targets.append(target_group)\n\n\nX = sequences\nY = np.array(targets)\n\nprint(\"{bags_count} bags\".format(bags_count=len(X)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1,\n                                                    random_state=42)\n\nX_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=0.25,\n                                                    random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = [item for sublist in X_train for item in sublist]\nY_train = [item for sublist in Y_train for item in sublist]\n\nX_validation = [item for sublist in X_validation for item in sublist]\nY_validation = [item for sublist in Y_validation for item in sublist]\n\nX_test = [item for sublist in X_test for item in sublist]\nY_test = [item for sublist in Y_test for item in sublist]\n\nprint(\"Train: {train_size}\\nValidation: {validation_size}\\nTest: {test_size}\\n\".format(train_size=len(X_train), validation_size=len(X_validation), test_size=len(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenizer\ntokenizer = Tokenizer(num_words=max_words)  \n\n\n\n# Updates internal vocabulary based on a list of texts. \n# This method creates the vocabulary index based on word frequency. \ntokenizer.fit_on_texts(X_train)\n\n\n# Transforms each row from texts to a sequence of integers. \n# So it basically takes each word in the text and replaces it \n# with its corresponding integer value from the\nX_train = tokenizer.texts_to_sequences(X_train)\nX_validation = tokenizer.texts_to_sequences(X_validation)\nX_test = tokenizer.texts_to_sequences(X_test)\n\n\n# Pad sequences\nX_train = pad_sequences(X_train, maxlen=max_sequence_length, dtype='int32', value=0)\nX_validation = pad_sequences(X_validation, maxlen=max_sequence_length, dtype='int32', value=0)\nX_test = pad_sequences(X_test, maxlen=max_sequence_length, dtype='int32', value=0)\n\n\nword_index = tokenizer.word_index\n\nX_train = np.array(X_train)\nY_train = np.array(Y_train)\nX_validation = np.array(X_validation)\nY_validation = np.array(Y_validation)\nX_test = np.array(X_test)\nY_test = np.array(Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.regularizers import l2\n\nl2_reg = l2(0.001)\n\ndef model_fn():\n    model = Sequential()\n\n    model.add(Embedding(max_words, embedding_size, input_length=max_sequence_length, embeddings_regularizer=l2_reg))\n    \n    #model.add(SpatialDropout1D(0.5))\n    model.add(Dropout(0.5))\n    \n    model.add(LSTM(32, dropout=0.5, recurrent_dropout=0.5, kernel_regularizer=l2_reg, recurrent_regularizer=l2_reg, bias_regularizer=l2_reg))\n    \n    model.add(Dropout(0.5))\n    \n    model.add(Dense(512, activation='relu'))\n\n    model.add(Dense(1, activation='sigmoid'))\n    \n    optimizer = RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0)\n    \n    model.compile(loss='binary_crossentropy',\n                  optimizer=optimizer,\n                  metrics=['accuracy'])\n\n    print(model.summary())\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm /kaggle/working/*","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pickle\n\n# epochs\nepochs = 10\n\n# number of samples to use for each gradient update\nbatch_size = 128\n\n# saving tokenizer\nwith open(tokenizer_file, 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\nmodel = model_fn()\n\n# loadin saved model\nif os.path.exists(model_file):\n    model.load_weights(model_file)\n\nhistory = model.fit(X_train, Y_train,\n          validation_data=(X_validation, Y_validation),\n          epochs=epochs,\n          batch_size=batch_size,\n          shuffle=True,\n          verbose=1)\n\n# saving model\nmodel.save_weights(model_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.title('Loss')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='validation')\nplt.legend()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Accuracy')\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='validation')\nplt.legend()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate model\nscores = model.evaluate(X_test, Y_test, verbose=0, batch_size=batch_size)\nprint(\"Acc: %.2f%%\" % (scores[1] * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the test data to create the submission.csv file\n\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n\n# Clear text\ntest['text'] = test['text'].apply(lambda x: clean_str(x))\n\n# Remove stop words\ntest['text'] = test['text'].apply(lambda x: \" \".join(remove_stopwords(x.split(\" \"))))\n\n# Get text\ntest_X = list(test[\"text\"])\n\n# Convert text to sequences\nsequences = tokenizer.texts_to_sequences(test_X)\n\n# Pad sequences\nsequences = pad_sequences(sequences,\n                             maxlen=max_sequence_length,\n                             dtype='int32',\n                             value=0)\n\n# Predict sequences\npredicted = model.predict(sequences)\n\nbinary_predicted = np.array(predicted) >= 0.5\ntargets = binary_predicted.astype(int).reshape((len(binary_predicted)))\n    \nmy_submission = pd.DataFrame({'id': test.id, 'target': targets})\nmy_submission.to_csv('submission.csv', index=False)\n\nprint(\"Submission file created!\")","execution_count":35,"outputs":[{"output_type":"stream","text":"Submission file created!\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}